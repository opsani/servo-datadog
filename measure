#!/usr/bin/env python3
import json
import os
import sys
import time

import yaml
from datadog import initialize
from datadog import api as dog
from datadog.api.exceptions import ApiError

from measure import Measure, ST_FAILED

DESC="Datadog measure driver for Opsani Optune"
VERSION="1.0.0"
HAS_CANCEL=True
PROGRESS_INTERVAL=30

DFLT_WARMUP     = 0
DFLT_DURATION   = 120
DFLT_TIME_AGGR  = 'avg'
DFLT_SPACE_AGGR = 'avg'
AGGR_OPERATORS  = ('avg', 'max', 'min', 'sum', 'raw')
API_KEY_FPATH   = os.getenv('OPTUNE_DATADOG_API_KEY_PATH', '/etc/optune-datadog-auth/api_key')
APP_KEY_FPATH   = os.getenv('OPTUNE_DATADOG_APP_KEY_PATH', '/etc/optune-datadog-auth/app_key')
CONFIG_FPATH    = os.getenv('CONFIG_FPATH', './config.yaml')


def inst_id(metric, metric_name, group_name):
    """get a suitable instance ID from a datadog series record.
    The 'scope' value is taken and post-processed to make it shorter if it turns out to be coming from a k8s cluster"""

    #
    scope = metric.get("scope")
    assert scope, 'Metric "{}" has to be grouped by some property, so we can define ' \
                  'it\'s multi-instance result ids'.format(metric_name)

    # try to interpret as tag:value,tag:value, ...
    # e.g.: kube_deployment:c1,kube_namespace:lk
    tags = dict([tag.split(':', 1) for tag in scope.split(',')])

    assert group_name in tags, 'Cannot find group {} in response on metric "{}"'.format(group_name, metric_name)

    return tags.get(group_name)


class Datadog(Measure):

    @staticmethod
    def get_config(path=CONFIG_FPATH):
        try:
            config = yaml.safe_load(open(path))['datadog']
            assert config and config.get('metrics'), 'No metrics has been provided at path "datadog.metrics" ' \
                                                     'in file located at {}'.format(path)
            return config
        except yaml.YAMLError as e:
            e.__cause__ = 'Error parsing config file located at {}: {}'.format(path, e.__cause__)
            raise
        except KeyError:
            raise KeyError('No `datadog` configuration has been provided in config file located '
                           'at {}'.format(path))

    # overwrites super
    def describe(self):
        # get dict of metrics active in the last 24 hours
        self.init_datadog()
        config = self.get_config()
        metrics = {}
        for name, metric in config['metrics'].items():
            metrics[name] = {'unit': metric.get('unit', '')}
        return metrics

    # overwrites super
    def handle_cancel(self, signal, frame):
        err = 'Exiting due to signal: {}'.format(signal)
        self.print_measure_error(err, ST_FAILED)

        sys.exit(3)

    # overwrites super
    def measure(self):
        config = self.get_config()
        # parse and valcheck control configuration
        assert 'control' in self.input_data, 'Input data missing control configuration'
        control  = self.input_data['control']
        warmup   = int(control.get('warmup', DFLT_WARMUP))
        duration = int(control.get('duration', DFLT_DURATION))
        assert warmup >= 0 and duration >= 0, \
            'Both warmup {} and duration {} must be non-negative'.format(warmup, duration)

        self.init_datadog()

        for metric_name, metric in config['metrics'].items():
            group_name = metric['use_as_instance_id']
            assert group_name, 'Property "use_as_instance_id" for metric "{}" has to be set. ' \
                               'It tells the driver what scope parameter to use when defining ' \
                               'instance id for particular metric.'.format(metric_name)
            query = metric['query'].replace(' ', '')

            # query datadog:  verify query is valid
            qval = self.query_datadog_metric(query, duration, metric_name, group_name)
            self.debug('Initial value for query {}:  {}'.format(query, qval))

        # sleep
        self.t_sleep = warmup + duration
        self.debug('Sleeping for {:d} seconds ({:d} warmup + {:d} duration)'.format(self.t_sleep, warmup, duration))
        time.sleep(self.t_sleep)

        metrics = {}
        for metric_name, metric in config['metrics'].items():
            group_name = metric['use_as_instance_id']
            assert group_name, 'Property "use_as_instance_id" for metric "{}" has to be set. ' \
                               'It tells the driver what scope parameter to use when defining ' \
                               'instance id for particular metric.'.format(metric_name)
            query = metric['query'].replace(' ', '')

            # query datadog:  measure
            qval = self.query_datadog_metric(query, duration, metric_name, group_name)

            # construct result
            metrics.update({
                metric_name: {
                    'values': qval,
                    'annotation': query,
                }
            })

        annotations = {}

        return metrics, annotations

    # overwrites super:  update progress before printing it
    def print_progress(
            self,
            message=None,
            msg_index=None,
            stage=None,
            stageprogress=None):

        # update progress based on how much time has elapsed
        t_taken = time.time() - self.t_measure_start
        self.progress = int(min(100.0, 100.0*((t_taken)/self.t_sleep)))
        super().print_progress(message, msg_index, stage, stageprogress)

    # helper:  initialize datadog api module
    def init_datadog(self):

        # as required:  read api_key and app_key from files (e.g., from k8s
        # secret volume mounted files)
        if not hasattr(self, 'datadog_api_key'):
            assert os.path.isfile(API_KEY_FPATH), \
                'Datadog api_key file {} does not exist'.format(API_KEY_FPATH)
            with open(API_KEY_FPATH, 'r') as f:
                self.datadog_api_key = f.read().rstrip()
        if not hasattr(self, 'datadog_app_key'):
            assert os.path.isfile(APP_KEY_FPATH), \
                'Datadog app_key file {} does not exist'.format(APP_KEY_FPATH)
            with open(APP_KEY_FPATH, 'r') as f:
                self.datadog_app_key = f.read().rstrip()

        # initialize datadog api module
        options = {
            'api_key': self.datadog_api_key,
            'app_key': self.datadog_app_key,
            'mute': False,
        }
        initialize(**options)
        # try:
        #     initialize(**options)
        #     try:
        #         dog.Metric.query(start=0, end=1, query='system.cpu.user{*}')
        #     except ApiError as e:
        #         e.args += ('Unable to confirm connectivity to Datadog API servers.',)
        #         raise
        # except Exception as e:
        #     raise Exception('Failed to initialize Datadog API module. Error: {}'.format(str(e)))

    # helper:  query datadog metrics over last duration seconds and return
    # value computed from aggregating in time and space
    def query_datadog_metric(self, query, duration, metric_name, group_name):
        # query datadog metrics
        try:
            now = int(time.time())
            data = dog.Metric.query(start=now - duration, end=now, query=query)
        except Exception as e:
            raise Exception('Failed to query Datadog for metric {}. Error: {}'.format(query, str(e)))

        # verify query status and response type
        q_status = data.get('status')
        assert q_status == 'ok', \
            'Datadog query {} returned with status {} errors {}'.format(query, q_status, data.get('errors'))
        q_res_type = data.get('res_type')
        assert q_res_type == 'time_series', \
            'Datadog query {} res_type is not time_series but {}'.format(query, q_res_type)

        d = []
        for s in data.get('series', []):
            d.append({'id': inst_id(s, metric_name, group_name), 'data': [[int(p[0]), p[1]] for p in s['pointlist']]})

        return d


if __name__ == '__main__':
    dd = Datadog(VERSION, DESC, HAS_CANCEL, PROGRESS_INTERVAL)
    dd.run()
