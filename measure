#!/usr/bin/env python3

import os
import signal
import subprocess
import sys
import time

from datadog import initialize
from datadog import api as dog

from measure import Measure, ST_FAILED

DESC="Datadog measure driver for Opsani Optune"
VERSION="1.0.0"
HAS_CANCEL=True
PROGRESS_INTERVAL=30

DFLT_WARMUP     = 0
DFLT_DURATION   = 120
DFLT_TIME_AGGR  = 'avg'
DFLT_SPACE_AGGR = 'avg'
AGGR_OPERATORS  = ('avg', 'max', 'min', 'sum', 'raw')
API_KEY_FPATH   = '/etc/optune-datadog-auth/api_key'
APP_KEY_FPATH   = '/etc/optune-datadog-auth/app_key'

def inst_id(metric):
    """get a suitable instance ID from a datadog series record. The 'scope' value is taken and post-processed to make it shorter if it turns out to be coming from a k8s cluster"""

    #
    i = s.get("scope", "")
    if not i: # we have no alternative to scope for now, return empty
        return i

    #kube_deployment:c1,kube_namespace:lk
    a = i.split(",")
    if not a[0].startswith("kube_deployment:"):
        return i

    ns = ""
    dep = a[0][len("kube_deployment:"):]
    if len(a) > 1 and a[1].startswith("kube_namespace:"):
        ns = a[1][len("kube_namespace:"):]+"."

    return "dep:" + ns + dep

class Datadog(Measure):

    # overwrites super
    def describe(self):

        # get dict of metrics active in the last 24 hours
        self._init_datadog()
        from_time = int(time.time()) - (60 * 60 * 24)
        data = dog.Metric.list(from_time)
        self.debug('Datadog describe response: ', data)

        # validate and return metrics
        assert 'metrics' in data, \
            'Datadog server did not return list of active metrics'
        assert len(data['metrics']) > 0, \
            'Datadog server returned empty list of metrics'
        metrics = {m:{} for m in data['metrics']}
        return metrics

    # overwrites super
    def handle_cancel(self, signal, frame):
        err = 'Exiting due to signal: {}'.format(signal)
        self.print_measure_error(err, ST_FAILED)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        sys.exit(3)

    # overwrites super
    def measure(self):

        # parse and valcheck control configuration
        assert 'control' in self.input_data, 'Input data missing control configuration'
        control  = self.input_data['control']
        warmup   = int(control.get('warmup', DFLT_WARMUP))
        duration = int(control.get('duration', DFLT_DURATION))
        assert warmup >= 0 and duration >= 0, \
            'Both warmup {} and duration {} must be non-negative'.format(warmup, duration)
        assert 'userdata' in control, 'Control configuration missing userdata'
        userdata = control['userdata']
        assert 'query' in userdata, 'Query missing from control configuration userdata'
        query = userdata['query'].replace(' ','')
        time_aggr = userdata.get('time_aggr', DFLT_TIME_AGGR)
        assert time_aggr in AGGR_OPERATORS, \
            'Unknown time_aggr {}'.format(time_aggr)
        space_aggr  = userdata.get('space_aggr', DFLT_SPACE_AGGR)
        assert space_aggr in AGGR_OPERATORS, \
            'Unknown space_aggr {}'.format(space_aggr)
        pre_cmd_async = userdata.get('pre_cmd_async')
        pre_cmd  = userdata.get('pre_cmd')
        post_cmd = userdata.get('post_cmd')

        # query datadog:  verify query is valid
        self._init_datadog()
        qval = self._query_datadog_metric(query, time_aggr, space_aggr, duration)
        self.debug('Initial value for query {}:  {}'.format(query, qval))

        # execute pre_cmd_async, if any
        if pre_cmd_async is not None:
            self.proc_async = self._run_command_async(pre_cmd_async)

        # execute pre_cmd, if any
        if pre_cmd is not None:
            self._run_command(pre_cmd, pre=True)

        # sleep
        self.t_sleep = warmup + duration
        self.debug('Sleeping for {:d} seconds ({:d} warmup + {:d} duration)'.format(
            self.t_sleep, warmup, duration))
        time.sleep(self.t_sleep)

        # query datadog:  measure
        qval = self._query_datadog_metric(query, time_aggr, space_aggr, duration)

        # execute post_cmd, if any
        if post_cmd is not None:
            self._run_command(post_cmd, pre=False)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        # construct result:  use computed value for perf metric
        val_key = "values" if space_aggr == "raw" else "value"
        metrics = {
            'perf': {
                val_key : qval,
                'annotation': query,
            }
        }
        annotations = {}

        return metrics, annotations

    # overwrites super:  update progress before printing it
    def print_progress(
            self,
            message=None,
            msg_index=None,
            stage=None,
            stageprogress=None):

        # update progress based on how much time has elapsed
        t_taken = time.time() - self.t_measure_start
        self.progress = int(min(100.0, 100.0*((t_taken)/self.t_sleep)))
        super().print_progress(message, msg_index, stage, stageprogress)

    # helper:  initialize datadog api module
    def _init_datadog(self):

        # as required:  read api_key and app_key from files (e.g., from k8s
        # secret volume mounted files)
        if not hasattr(self, 'datadog_api_key'):
            assert os.path.isfile(API_KEY_FPATH), \
                'Datadog api_key file {} does not exist'.format(API_KEY_FPATH)
            with open(API_KEY_FPATH, 'r') as f:
                self.datadog_api_key = f.read().rstrip()
        if not hasattr(self, 'datadog_app_key'):
            assert os.path.isfile(APP_KEY_FPATH), \
                'Datadog app_key file {} does not exist'.format(APP_KEY_FPATH)
            with open(APP_KEY_FPATH, 'r') as f:
                self.datadog_app_key = f.read().rstrip()

        # initialize datadog api module
        options = {
            'api_key': self.datadog_api_key,
            'app_key': self.datadog_app_key
        }
        try:
            initialize(**options)
        except Exception as e:
            raise Exception(
                'Failed to initialize Datadog API module. Error: {}'.format(str(e)))

    # helper:  query datadog metrics over last duration seconds and return
    # value computed from aggregating in time and space
    def _query_datadog_metric(self, query, time_aggr, space_aggr, duration):

        # query datadog metrics
        try:
            now = int(time.time())
            data = dog.Metric.query(start=now - duration, end=now, query=query)
        except Exception as e:
            raise Exception(
                'Failed to query Datadog for metric {}. Error: {}'.format(
                    query, str(e)))

        # verify query status and response type
        q_status = data.get('status')
        assert q_status == 'ok', \
            'Datadog query {} returned with status {} errors {}'.format(query,
            q_status, data.get('errors'))
        q_res_type = data.get('res_type')
        assert q_res_type == 'time_series', \
            'Datadog query {} res_type is not time_series but {}'.format(
            query, q_res_type)

        if space_aggr == "raw": # send metric data as-is, let server-side aggregate
            d = []
            for s in data.get('series',[]):
                d.append( {"id": inst_id(s), "data" : s["pointlist"]} )
            return d

        # get list of metric series from data
        m_series = data.get('series',[])
        assert len(m_series) >= 1, \
            'Datadog query {} returned {} time series pointlists'.format(
            query, len(m_series))

        # aggregate in time
        time_aggr_vals = []
        for s in m_series:
            plist = s.get('pointlist',[])
            assert len(plist) > 0, \
                'Datadog query {} returned empty pointlist for expression {}'.format(
                query, len(plist), s.get('expression'))

            # extract just values from pointlist, aggregate, and save
            # note:  i[0] is the time in UTC milliseconds
            pvals = [i[1] for i in plist]
            time_aggr_vals.append(self._aggregate_values(pvals, time_aggr))

        # aggregate in space and return value
        return self._aggregate_values(time_aggr_vals, space_aggr)

    # helper:  return an aggregate value computed from an input list of values
    def _aggregate_values(self, vals, aggr):
        if aggr == 'avg':
            return sum(vals) / float(len(vals))
        if aggr == 'max':
            return max(vals)
        if aggr == 'min':
            return min(vals)
        if aggr == 'sum':
            return sum(vals)
        raise Exception('Unexpected aggregation method {}'.format(aggr))

    # helper:  run a Bash shell command and raise an Exception on failure
    # note:  if cmd is a string, this supports shell pipes, environment variable
    # expansion, etc.  The burden of safety is entirely on the user.
    def _run_command(self, cmd, pre=True):
        cmd_type = 'Pre-command' if pre else 'Post-command'
        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            shell=True, executable='/bin/bash')
        msg = "cmd '{}', exit code {}, stdout {}, stderr {}".format(cmd,
            res.returncode, res.stdout, res.stderr)
        assert res.returncode == 0, '{} failed:  {}'.format(cmd_type, msg)
        self.debug('{}:  {}'.format(cmd_type, msg))

    # helper:  run a Bash shell command with stdout/stderr directed to /dev/null
    # and return the popen object
    def _run_command_async(self, cmd):
        proc = subprocess.Popen(cmd, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL, shell=True, executable='/bin/bash',
            preexec_fn=os.setpgrp)
        self.debug('Pre-command async:  {}'.format(cmd))
        return proc


if __name__ == '__main__':
    dd = Datadog(VERSION, DESC, HAS_CANCEL, PROGRESS_INTERVAL)
    dd.run()
